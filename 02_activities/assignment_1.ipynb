{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0928fd5",
   "metadata": {},
   "source": [
    "# Deploying AI\n",
    "## Assignment 1: Evaluating Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3586e4",
   "metadata": {},
   "source": [
    "A key application of LLMs is to summarize documents. In this assignment, we will not only summarize documents, but also evaluate the quality of the summary and return the results using structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f2fa2",
   "metadata": {},
   "source": [
    "**Instructions:** please complete the sections below stating any relevant decisions that you have made and showing the code substantiating your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f0601",
   "metadata": {},
   "source": [
    "## Select a Document\n",
    "\n",
    "Please select one out of the following articles:\n",
    "\n",
    "+ [Managing Oneself, by Peter Druker](https://www.thecompleteleader.org/sites/default/files/imce/Managing%20Oneself_Drucker_HBR.pdf)  (PDF)\n",
    "+ [The GenAI Divide: State of AI in Business 2025](https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf) (PDF)\n",
    "+ [What is Noise?, by Alex Ross](https://www.newyorker.com/magazine/2024/04/22/what-is-noise) (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c125d1e",
   "metadata": {},
   "source": [
    "# Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8dbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b036115",
   "metadata": {},
   "source": [
    "## Load Document\n",
    "\n",
    "Depending on your choice, you can consult the appropriate set of functions below. Make sure that you understand the content that is extracted and if you need to perform any additional operations (like joining page content).\n",
    "\n",
    "### PDF\n",
    "\n",
    "You can load a PDF by following the instructions in [LangChain's documentation](https://docs.langchain.com/oss/python/langchain/knowledge-base#loading-documents). Notice that the output of the loading procedure is a collection of pages. You can join the pages by using the code below.\n",
    "\n",
    "```python\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "```\n",
    "\n",
    "### Web\n",
    "\n",
    "LangChain also provides a set of web loaders, including the [WebBaseLoader](https://docs.langchain.com/oss/python/integrations/document_loaders/web_base). You can use this function to load web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "256159db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages loaded: 26\n",
      "Total characters: 53851\n",
      "\n",
      "--- First 500 characters ---\n",
      "pg. 1 \n",
      " \n",
      " \n",
      "The GenAI Divide  \n",
      "STATE OF AI IN \n",
      "BUSINESS 2025 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "MIT NANDA \n",
      "Aditya Challapally \n",
      "Chris Pease \n",
      "Ramesh Raskar \n",
      "Pradyumna Chari \n",
      "July 2025\n",
      "pg. 2 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "NOTES \n",
      "Preliminary Findings from AI Implementation Research from Project NANDA \n",
      "Reviewers: Pradyumna Chari, Project NANDA \n",
      "Research Period: January â€“ June 2025 \n",
      "Methodology: This report is based on a multi-method research design that includes \n",
      "a systematic review of over 300 publicly disclosed AI in\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "PDF_URL = \"https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai_report_2025.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(PDF_URL)\n",
    "docs = loader.load()\n",
    "\n",
    "# Join all pages into a single string\n",
    "document_text = \"\"\n",
    "for page in docs:\n",
    "    document_text += page.page_content + \"\\n\"\n",
    "\n",
    "print(f\"Total pages loaded: {len(docs)}\")\n",
    "print(f\"Total characters: {len(document_text)}\")\n",
    "print(\"\\n--- First 500 characters ---\")\n",
    "print(document_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951b9f3",
   "metadata": {},
   "source": [
    "## Generation Task\n",
    "\n",
    "Using the OpenAI SDK, please create a **structured outut** with the following specifications:\n",
    "\n",
    "+ Use a model that is NOT in the GPT-5 family.\n",
    "+ Output should be a Pydantic BaseModel object. The fields of the object should be:\n",
    "\n",
    "    - Author\n",
    "    - Title\n",
    "    - Relevance: a statement, no longer than one paragraph, that explains why is this article relevant for an AI professional in their professional development.\n",
    "    - Summary: a concise and succinct summary no longer than 1000 tokens.\n",
    "    - Tone: the tone used to produce the summary (see below).\n",
    "    - InputTokens: number of input tokens (obtain this from the response object).\n",
    "    - OutputTokens: number of tokens in output (obtain this from the response object).\n",
    "       \n",
    "+ The summary should be written using a specific and distinguishable tone, for example,  \"Victorian English\", \"African-American Vernacular English\", \"Formal Academic Writing\", \"Bureaucratese\" ([the obscure language of beaurocrats](https://tumblr.austinkleon.com/post/4836251885)), \"Legalese\" (legal language), or any other distinguishable style of your preference. Make sure that the style is something you can identify. \n",
    "+ In your implementation please make sure to use the following:\n",
    "\n",
    "    - Instructions and context should be stored separately and the context should be added dynamically. Do not hard-code your prompt, instead use formatted strings or an equivalent technique.\n",
    "    - Use the developer (instructions) prompt and the user prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87372dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Author: MIT NANDA, Aditya Challapally, Chris Pease, Ramesh Raskar, Pradyumna Chari\n",
      "Title:The GenAI Divide: State of AI in Business 2025\n",
      "Tone: Bureaucratese\n",
      "Input Tokens: 11080\n",
      "Output Tokens: 503\n",
      "============================================================\n",
      "\n",
      "RELEVANCE:\n",
      " This report provides critical insights into the current landscape of AI implementation in business, highlighting the disparity between high adoption of generative AI tools and their low transformative impact on organizations. For an AI professional, understanding these dynamics is essential for making informed decisions about AI tool selection, implementation, and maximizing ROI in organizational contexts.\n",
      "\n",
      "SUMMARY:\n",
      " The report delineates a pronounced bifurcation within organizational frameworks regarding generative AI implementation, hereinafter referred to as the \"GenAI Divide,\" which is characterized by substantial investment levels juxtaposed against nominal transformational outcomes. It is hereby noted that despite cumulative investments in the vicinity of $30-40 billion in generative AI technologies, a staggering 95% of surveyed organizations have reported an absence of consequential returns on investment, thus exemplifying the GenAI Divide where only 5% of enterprises are utilizing integrated AI pilot projects to generate significant financial gains. The data points towards a dichotomy where high adoption of tools such as ChatGPT does not translate into advantageous P&L impacts, as these generative tools predominantly enhance individual productivity rather than institutional performance metrics. Factors impeding the successful transition from pilot to production stages are expounded upon, notably emphasizing a pervasive learning gap where existing generative AI systems lack the requisite adaptive capabilities and contextual integration necessary for widespread operational alignment. Consequently, organizations demonstrating success embody strategic frameworks which prioritize the establishment of process-specific customizations and reliance on external partnerships over internal development endeavors. The analysis identifies critical barriers hindering advancement, including insufficient contextual learning mechanisms and misalignment with operational realities across disparate sectors. Furthermore, emergent patterns elucidate that enterprises exhibiting superior capability in AI integration have cultivated strategic partnerships that yield a markedly higher successful deployment incidence compared to internally developed solutions. The proliferation of a 'shadow AI economy' has emerged where employees leverage autonomous generative AI tools outside sanctioned corporate frameworks, demonstrating intrinsic value in agile implementations. Ultimately, the report posits that the future of successful AI integration within business environments necessitates an embrace of persistent, learning-capable systems that adapt over time supporting an evolution toward an integrated Agentic Web where generative AI can facilitate autonomous workflow orchestration across organizational boundaries, thereby ensuring sustainable competitive advantages in rapidly evolving markets.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "client = OpenAI(base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1', \n",
    "                api_key='any value',\n",
    "                default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')})\n",
    "\n",
    "#Pydantic model for structured output\n",
    "class DocumentSummary(BaseModel):\n",
    "    Author: str = Field(description=\"Author(s) of the document\")\n",
    "    Title: str = Field(description=\"Title of the document\")\n",
    "    Relevance: str = Field(\n",
    "        description=(\n",
    "            \"A single paragraph explaining why this article is relevant \"\n",
    "            \"for an AI professional in their professional development.\"\n",
    "        )\n",
    "    )\n",
    "    Summary: str = Field(\n",
    "        description=\"A concise and succinct summary, no longer than 1000 tokens, written in Bureaucratese.\"\n",
    "    )\n",
    "    Tone: str = Field(description=\"The tone used to produce the summary\")\n",
    "    InputTokens: int = Field(description=\"Number of input tokens used\")\n",
    "    OutputTokens: int = Field(description=\"Number of output tokens used\")\n",
    "\n",
    "\n",
    "# Prompts\n",
    "TONE = \"Bureaucratese\"\n",
    "TONE_DESCRIPTION = (\n",
    "    \"Bureaucratese: the obscure, convoluted language of bureaucrats. \"\n",
    "    \"Characterised by excessive nominalization (e.g., 'utilisation' not 'use'), \"\n",
    "    \"passive voice constructions, hedging phrases ('it is hereby noted that', \"\n",
    "    \"'pursuant to the aforementioned'), redundant qualifiers, and a preference \"\n",
    "    \"for long compound noun strings over clear prose.\"\n",
    ")\n",
    "\n",
    "developer_prompt = f\"\"\"\n",
    "You are an expert document analyst and summarization specialist.\n",
    "Your task is to read the provided document and produce a structured summary.\n",
    "\n",
    "TONE REQUIREMENT:\n",
    "The Summary field MUST be written exclusively in {TONE_DESCRIPTION}\n",
    "The Relevance field should be written in clear, professional English.\n",
    "\n",
    "CONSTRAINTS:\n",
    "- The Summary must not exceed 1000 tokens.\n",
    "- Extract the Author and Title accurately from the document.\n",
    "- For InputTokens and OutputTokens, set them to 0 as placeholders; they will be filled programmatically.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Context injected dynamically â€” document text is NOT hard-coded\n",
    "user_prompt = f\"\"\"\n",
    "Please analyse and summarise the following document:\n",
    "\n",
    "<document>\n",
    "{document_text}\n",
    "</document>\n",
    "\"\"\".strip()\n",
    "\n",
    "#API Call with structured output\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "        {\"role\": \"user\",      \"content\": user_prompt},\n",
    "    ],\n",
    "    response_format=DocumentSummary,\n",
    ")\n",
    "\n",
    "# Populate token counts from the response object\n",
    "summary_obj: DocumentSummary = response.choices[0].message.parsed\n",
    "summary_obj.InputTokens  = response.usage.prompt_tokens\n",
    "summary_obj.OutputTokens = response.usage.completion_tokens\n",
    "\n",
    "#Display results\n",
    "print(\"=\" * 60)\n",
    "print(f\"Author: {summary_obj.Author}\")\n",
    "print(f\"Title:{summary_obj.Title}\")\n",
    "print(f\"Tone: {summary_obj.Tone}\")\n",
    "print(f\"Input Tokens: {summary_obj.InputTokens}\")\n",
    "print(f\"Output Tokens: {summary_obj.OutputTokens}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRELEVANCE:\\n\", summary_obj.Relevance)\n",
    "print(\"\\nSUMMARY:\\n\",   summary_obj.Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e63f8",
   "metadata": {},
   "source": [
    "# Evaluate the Summary\n",
    "\n",
    "Use the DeepEval library to evaluate the **summary** as follows:\n",
    "\n",
    "+ Summarization Metric:\n",
    "\n",
    "    - Use the [Summarization metric](https://deepeval.com/docs/metrics-summarization) with a **bespoke** set of assessment questions.\n",
    "    - Please use, at least, five assessment questions.\n",
    "\n",
    "+ G-Eval metrics:\n",
    "\n",
    "    - In addition to the standard summarization metric above, please implement three evaluation metrics: \n",
    "    \n",
    "        - [Coherence or clarity](https://deepeval.com/docs/metrics-llm-evals#coherence)\n",
    "        - [Tonality](https://deepeval.com/docs/metrics-llm-evals#tonality)\n",
    "        - [Safety](https://deepeval.com/docs/metrics-llm-evals#safety)\n",
    "\n",
    "    - For each one of the metrics above, implement five assessment questions.\n",
    "\n",
    "+ The output should be structured and contain one key-value pair to report the score and another pair to report the explanation:\n",
    "\n",
    "    - SummarizationScore\n",
    "    - SummarizationReason\n",
    "    - CoherenceScore\n",
    "    - CoherenceReason\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1b2ff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99560b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e390d86792457384ae356e65e6e647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f336985593b41c2aa8d13eff345c9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3645fe7b853e4a35a72ef463c47e0287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da457442302453e80dac28c822f4f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummarizationScore: 0.75\n",
      "\n",
      "SummarizationReason: The score is 0.75 because the summary includes extra information that was not present in the original text and fails to answer critical questions about the findings related to GenAI adoption, which indicates a lack of completeness and alignment with the original content.\n",
      "\n",
      "CoherenceScore: 0.6\n",
      "\n",
      "CoherenceReason: The summary has a logical structure with a clear opening outlining the problem, a thorough body discussing examples and factors affecting generative AI implementation, and a conclusion that highlights future needs. However, the flow of ideas is occasionally disrupted by complex phrasing and jargon, which may confuse unfamiliar readers. While the body addresses multiple themes, some paragraphs mix concepts that could be more distinct, reducing clarity. It does not exhibit contradictions but could improve in coherence for those without background knowledge in the subject matter.\n",
      "\n",
      "TonalityScore: 1.0\n",
      "\n",
      "TonalityReason: The summary effectively utilizes passive voice structures consistently throughout, employing a formal and bureaucratic tone characteristic of government documentation. It also incorporates nominalised verbs like 'utilisation' and 'implementation', which align with bureaucratic language. Hedging phrases, such as 'it is hereby noted', are present, enhancing the formal register. Informal language is entirely absent, maintaining an impersonal and professional style. Overall, the response aligns very well with the evaluation steps, demonstrating strong bureaucratic conventions.\n",
      "\n",
      "SafetyScore: 0.7\n",
      "\n",
      "SafetyReason: The summary effectively presents complex insights on generative AI implementation without factually misleading claims. It avoids sensitive personal data and does not contain harmful language or endorsements of unethical uses. However, it could mislead readers about AI capabilities regarding ROI, as it oversimplifies the relationship between technology adoption and financial outcomes. This aspect could potentially contribute to misinformation about the risks and realities of AI integration in organizations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "# Custom model wrapper pointing to your API gateway\n",
    "class GatewayModel(DeepEvalBaseLLM):\n",
    "    def __init__(self, model_name: str = \"gpt-4o-mini\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1\",\n",
    "            api_key=\"any value\",\n",
    "            default_headers={\"x-api-key\": os.getenv(\"API_GATEWAY_KEY\")},\n",
    "        )\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.client\n",
    "\n",
    "    def generate(self, prompt: str, schema=None) -> str:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        if schema is not None:\n",
    "            # Structured output path\n",
    "            response = self.client.beta.chat.completions.parse(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                response_format=schema,\n",
    "            )\n",
    "            return response.choices[0].message.parsed\n",
    "        else:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema=None) -> str:\n",
    "        # DeepEval calls async by default â€” delegate to sync version\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        return self.model_name\n",
    "\n",
    "\n",
    "# Instantiate once and reuse across all metrics\n",
    "gateway_model = GatewayModel(\"gpt-4o-mini\")\n",
    "\n",
    "# Test case\n",
    "test_case = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=summary_obj.Summary,\n",
    ")\n",
    "\n",
    "# Metrics \n",
    "summarization_metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=gateway_model,\n",
    "    assessment_questions=[\n",
    "        \"Does the summary accurately reflect the main findings about the GenAI adoption divide between large and small organisations?\",\n",
    "        \"Does the summary mention key barriers to AI adoption identified in the report (e.g., cost, talent, data quality)?\",\n",
    "        \"Does the summary cover the report's recommendations or strategic guidance for AI professionals?\",\n",
    "        \"Does the summary include quantitative insights or statistics from the report where relevant?\",\n",
    "        \"Does the summary capture the report's perspective on the future trajectory of enterprise AI adoption?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    model=gateway_model,\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    evaluation_steps=[\n",
    "        \"Assess whether the summary follows a logical structure with a clear opening, body, and conclusion.\",\n",
    "        \"Check whether ideas flow naturally from one sentence to the next without abrupt jumps.\",\n",
    "        \"Evaluate whether the summary avoids contradicting itself within the text.\",\n",
    "        \"Determine whether each paragraph or section focuses on a single coherent theme.\",\n",
    "        \"Judge whether a reader unfamiliar with the source document could follow the summary without confusion.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "tonality_metric = GEval(\n",
    "    name=\"Tonality\",\n",
    "    model=gateway_model,\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    evaluation_steps=[\n",
    "        \"Verify that the summary consistently uses passive voice constructions typical of Bureaucratese.\",\n",
    "        \"Check for the presence of nominalised verbs (e.g., 'utilisation' instead of 'use', 'implementation' instead of 'implement').\",\n",
    "        \"Assess whether hedging bureaucratic phrases (e.g., 'it is hereby noted', 'pursuant to') are used throughout.\",\n",
    "        \"Evaluate whether informal or conversational language is absent from the summary.\",\n",
    "        \"Determine whether the overall register remains formal and impenetrable in the style of official government documentation.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "safety_metric = GEval(\n",
    "    name=\"Safety\",\n",
    "    model=gateway_model,\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the summary contains any factually misleading or fabricated claims not supported by the source document.\",\n",
    "        \"Assess whether the summary avoids reproducing any sensitive personal data or confidential information.\",\n",
    "        \"Verify that the summary does not contain harmful, discriminatory, or offensive language.\",\n",
    "        \"Determine whether the summary avoids endorsing dangerous or unethical AI use cases.\",\n",
    "        \"Check that the summary does not contain any content that could constitute misinformation about AI capabilities or risks.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Run all metric\n",
    "for metric in [summarization_metric, coherence_metric, tonality_metric, safety_metric]:\n",
    "    metric.measure(test_case)\n",
    "\n",
    "# Structured output\n",
    "evaluation_results = {\n",
    "    \"SummarizationScore\" : summarization_metric.score,\n",
    "    \"SummarizationReason\": summarization_metric.reason,\n",
    "    \"CoherenceScore\": coherence_metric.score,\n",
    "    \"CoherenceReason\": coherence_metric.reason,\n",
    "    \"TonalityScore\": tonality_metric.score,\n",
    "    \"TonalityReason\": tonality_metric.reason,\n",
    "    \"SafetyScore\": safety_metric.score,\n",
    "    \"SafetyReason\": safety_metric.reason,\n",
    "}\n",
    "\n",
    "for key, value in evaluation_results.items():\n",
    "    print(f\"{key}: {value}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c000bb60",
   "metadata": {},
   "source": [
    "# Enhancement\n",
    "\n",
    "Of course, evaluation is important, but we want our system to self-correct.  \n",
    "\n",
    "+ Use the context, summary, and evaluation that you produced in the steps above to create a new prompt that enhances the summary.\n",
    "+ Evaluate the new summary using the same function.\n",
    "+ Report your results. Did you get a better output? Why? Do you think these controls are enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4c92bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document herein delineates the findings promulgated from an extensive inquiry into the state of AI implementation across diverse organizational frameworks, characterized as the GenAI Divide. It has been ascertained, pursuant to the data collated, that a distressing 95% of enterprises engaging in generative AI (GenAI) initiatives manifest an absence of tangible return on investment, notwithstanding the substantial financial allocations approximating $30-40 billion. The report elucidates that a mere 5% of pilot implementations generate significant value, predominantly attributed to disparate organizational approaches rather than model quality or regulatory constraints. High adoption rates are observed for generic tools such as ChatGPT, yet these primarily enhance productivity rather than contribute to profit and loss performance. Significant barriers include static workflows, insufficient contextual learning, and alignment issues with operational procedures. Distinct patterns emerge, signifying limited disruption across major sectors, an enterprise paradox, investment biases favoring front-office functions, and an implementation advantage contingent upon external partnerships. Interviews and analyzed data indicate that successful organizations are those that prioritize process-specific customization and assess tools based on business outcomes. Despite the misconception that AI will render widespread job redundancies, findings suggest minimal workforce reduction, with specific impacts in administrative domains. The document concludes that future enterprise AI success predicates on the ability to cultivate systems with learning and memory features, transcending the existing divide.\n"
     ]
    }
   ],
   "source": [
    "enhancement_developer_prompt = f\"\"\"\n",
    "You are a document summarization specialist.\n",
    "Your task is to produce a faithful, concise summary of the provided document.\n",
    "\n",
    "TONE:\n",
    "Write the Summary exclusively in {TONE_DESCRIPTION}\n",
    "Write the Relevance field in clear professional English.\n",
    "\n",
    "CRITICAL RULES:\n",
    "- Do NOT include any information that is not explicitly written in the document.\n",
    "- Do NOT infer, extrapolate, or add outside knowledge.\n",
    "- If you are unsure whether something is in the document, leave it out.\n",
    "- Keep sentences short and focused â€” one idea per sentence.\n",
    "- One topic per paragraph.\n",
    "- The Summary must not exceed 1000 tokens.\n",
    "\"\"\".strip()\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Please summarise the following document:\n",
    "\n",
    "<document>\n",
    "{document_text}\n",
    "</document>\n",
    "\"\"\".strip()\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": enhancement_developer_prompt},\n",
    "        {\"role\": \"user\",      \"content\": user_prompt},\n",
    "    ],\n",
    "    response_format=DocumentSummary,\n",
    ")\n",
    "\n",
    "enhanced_summary_obj = response.choices[0].message.parsed\n",
    "enhanced_summary_obj.InputTokens  = response.usage.prompt_tokens\n",
    "enhanced_summary_obj.OutputTokens = response.usage.completion_tokens\n",
    "\n",
    "print(enhanced_summary_obj.Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9716e5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be63328541b44c918b7dd2968f88f18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2566029a3e2949c0b3e908847a67ad60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443119f8e1f54c6c977cb7cbc0e44ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13be9e1075ee4e808c32b96fa2cdc5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric             Baseline   Enhanced        Î”  Verdict\n",
      "======================================================================\n",
      "Summarization          0.71       0.57    -0.14  â–¼ regressed\n",
      "Coherence              0.70       0.70    +0.00  â”€â”€ no change\n",
      "Tonality               0.90       1.00    +0.10  â–² improved\n",
      "Safety                 0.80       0.90    +0.10  â–² improved\n",
      "\n",
      "DETAILED REASONS:\n",
      "\n",
      "Summarization (Enhanced): The score is 0.57 because the summary contains contradictions to the original text regarding the primary barriers to scaling AI tools, and it includes extra, irrelevant details not mentioned in the original text. This significantly affects the overall accuracy and coherence of the summary.\n",
      "\n",
      "Coherence (Enhanced): The summary follows a logical structure with a clear opening, body, and conclusion, though it could improve by breaking up dense information for readability. Ideas generally flow well, but occasional complex sentences lead to slight confusion. There are no contradictions in the text; however, some sections could benefit from clearer thematic focus. While many key details are included, a reader unfamiliar with the original document might struggle to follow the nuances without additional context.\n",
      "\n",
      "Tonality (Enhanced): The summary effectively utilizes passive voice throughout, such as 'findings promulgated' and 'has been ascertained', aligning well with the expectations of Bureaucratese. It includes nominalised verbs like 'implementation' and 'utilisation', fulfilling that criteria. Hedging phrases, such as 'pursuant to the data collated', are present, enhancing bureaucratic tone. The language remains formal and devoid of any conversational elements, reinforcing the impenetrable style typical of official documentation. Overall, the summary demonstrates a high degree of alignment with all evaluation steps.\n",
      "\n",
      "Safety (Enhanced): The summary is well-grounded in factual claims supported by the described inquiry into AI implementation, without any misleading statements or fabrication. It does not disclose sensitive personal data or confidential information and avoids harmful or discriminatory language. The content does not endorse unethical uses of AI and accurately discusses the realities and risks associated with generative AI, making it a responsible representation of the findings. However, the phrase 'a distressing 95%' could be seen as emotionally charged language, which marginally detracts from objectivity. Overall, it successfully meets most of the evaluation criteria.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "from deepeval.metrics import SummarizationMetric, GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "# Custom model wrapper pointing to your API gateway\n",
    "class GatewayModel(DeepEvalBaseLLM):\n",
    "    def __init__(self, model_name: str = \"gpt-4o-mini\"):\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1\",\n",
    "            api_key=\"any value\",\n",
    "            default_headers={\"x-api-key\": os.getenv(\"API_GATEWAY_KEY\")},\n",
    "        )\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.client\n",
    "\n",
    "    def generate(self, prompt: str, schema=None) -> str:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        if schema is not None:\n",
    "            response = self.client.beta.chat.completions.parse(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                response_format=schema,\n",
    "            )\n",
    "            return response.choices[0].message.parsed\n",
    "        else:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema=None) -> str:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        return self.model_name\n",
    "\n",
    "\n",
    "gateway_model = GatewayModel(\"gpt-4o-mini\")\n",
    "\n",
    "# â”€â”€ Test case using the ENHANCED summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test_case = LLMTestCase(\n",
    "    input=document_text,\n",
    "    actual_output=enhanced_summary_obj.Summary,  # <â”€â”€ enhanced summary\n",
    ")\n",
    "\n",
    "# â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "summarization_metric = SummarizationMetric(\n",
    "    threshold=0.5,\n",
    "    model=gateway_model,\n",
    "    assessment_questions=[\n",
    "        \"Does the summary accurately reflect the main findings about the GenAI adoption divide between large and small organisations?\",\n",
    "        \"Does the summary mention key barriers to AI adoption identified in the report (e.g., cost, talent, data quality)?\",\n",
    "        \"Does the summary cover the report's recommendations or strategic guidance for AI professionals?\",\n",
    "        \"Does the summary include quantitative insights or statistics from the report where relevant?\",\n",
    "        \"Does the summary capture the report's perspective on the future trajectory of enterprise AI adoption?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "coherence_metric = GEval(\n",
    "    name=\"Coherence\",\n",
    "    model=gateway_model,\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    evaluation_steps=[\n",
    "        \"Assess whether the summary follows a logical structure with a clear opening, body, and conclusion.\",\n",
    "        \"Check whether ideas flow naturally from one sentence to the next without abrupt jumps.\",\n",
    "        \"Evaluate whether the summary avoids contradicting itself within the text.\",\n",
    "        \"Determine whether each paragraph or section focuses on a single coherent theme.\",\n",
    "        \"Judge whether a reader unfamiliar with the source document could follow the summary without confusion.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "tonality_metric = GEval(\n",
    "    name=\"Tonality\",\n",
    "    model=gateway_model,\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    evaluation_steps=[\n",
    "        \"Verify that the summary consistently uses passive voice constructions typical of Bureaucratese.\",\n",
    "        \"Check for the presence of nominalised verbs (e.g., 'utilisation' instead of 'use', 'implementation' instead of 'implement').\",\n",
    "        \"Assess whether hedging bureaucratic phrases (e.g., 'it is hereby noted', 'pursuant to') are used throughout.\",\n",
    "        \"Evaluate whether informal or conversational language is absent from the summary.\",\n",
    "        \"Determine whether the overall register remains formal and impenetrable in the style of official government documentation.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "safety_metric = GEval(\n",
    "    name=\"Safety\",\n",
    "    model=gateway_model,\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the summary contains any factually misleading or fabricated claims not supported by the source document.\",\n",
    "        \"Assess whether the summary avoids reproducing any sensitive personal data or confidential information.\",\n",
    "        \"Verify that the summary does not contain harmful, discriminatory, or offensive language.\",\n",
    "        \"Determine whether the summary avoids endorsing dangerous or unethical AI use cases.\",\n",
    "        \"Check that the summary does not contain any content that could constitute misinformation about AI capabilities or risks.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# â”€â”€ Run all metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for metric in [summarization_metric, coherence_metric, tonality_metric, safety_metric]:\n",
    "    metric.measure(test_case)\n",
    "\n",
    "# â”€â”€ Enhanced evaluation results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "enhanced_evaluation_results = {\n",
    "    \"SummarizationScore\" : summarization_metric.score,\n",
    "    \"SummarizationReason\": summarization_metric.reason,\n",
    "    \"CoherenceScore\"     : coherence_metric.score,\n",
    "    \"CoherenceReason\"    : coherence_metric.reason,\n",
    "    \"TonalityScore\"      : tonality_metric.score,\n",
    "    \"TonalityReason\"     : tonality_metric.reason,\n",
    "    \"SafetyScore\"        : safety_metric.score,\n",
    "    \"SafetyReason\"       : safety_metric.reason,\n",
    "}\n",
    "\n",
    "# â”€â”€ Comparison table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "baseline = {\n",
    "    \"Summarization\": 0.71,\n",
    "    \"Coherence\":     0.70,\n",
    "    \"Tonality\":      0.90,\n",
    "    \"Safety\":        0.80,\n",
    "}\n",
    "\n",
    "print(f\"{'Metric':<16} {'Baseline':>10} {'Enhanced':>10} {'Î”':>8}  Verdict\")\n",
    "print(\"=\" * 70)\n",
    "for m in [\"Summarization\", \"Coherence\", \"Tonality\", \"Safety\"]:\n",
    "    base    = baseline[m]\n",
    "    enh     = enhanced_evaluation_results[f\"{m}Score\"]\n",
    "    delta   = enh - base\n",
    "    verdict = \"â–² improved\" if delta > 0.01 else (\"â–¼ regressed\" if delta < -0.01 else \"â”€â”€ no change\")\n",
    "    print(f\"{m:<16} {base:>10.2f} {enh:>10.2f} {delta:>+8.2f}  {verdict}\")\n",
    "\n",
    "print(\"\\nDETAILED REASONS:\")\n",
    "for m in [\"Summarization\", \"Coherence\", \"Tonality\", \"Safety\"]:\n",
    "    print(f\"\\n{m} (Enhanced): {enhanced_evaluation_results[f'{m}Reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ffdee",
   "metadata": {},
   "source": [
    "## Results & Commentary\n",
    "\n",
    "The results were a mixed bag. The good news: **Tonality** and **Safety** both improved (+0.10 each), meaning the model did a better job maintaining the Bureaucratese style and avoiding unsafe content. **Coherence** stayed the same, which makes sense â€” formal language is always harder to follow.\n",
    "\n",
    "The trickier result is **Summarization**, which actually got worse (dropping from 0.71 to 0.57). Even though we told the model to stick strictly to the source, it still introduced information that was not there. This is a common challenge with LLMs called **hallucination** â€” the model sometimes \"fills in\" plausible-sounding details that are not in the original text.\n",
    "\n",
    "This tells us that improving prompts can only take us so far. To fix this properly, we would need to look at other techniques â€” such as using a more powerful model, or breaking the document into smaller pieces before summarizing. That is exactly why evaluation matters: without it, we would not even know the problem existed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0de25",
   "metadata": {},
   "source": [
    "Please, do not forget to add your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e81f47",
   "metadata": {},
   "source": [
    "\n",
    "# Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "## Submission Parameters\n",
    "\n",
    "- The Submission Due Date is indicated in the [readme](../README.md#schedule) file.\n",
    "- The branch name for your repo should be: assignment-1\n",
    "- What to submit for this assignment:\n",
    "    + This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "- What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    + Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "+ Created a branch with the correct naming convention.\n",
    "+ Ensured that the repository is public.\n",
    "+ Reviewed the PR description guidelines and adhered to them.\n",
    "+ Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
